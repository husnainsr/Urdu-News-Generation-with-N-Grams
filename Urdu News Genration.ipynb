{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd097a40",
   "metadata": {},
   "source": [
    "### Article Generation using N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd8420",
   "metadata": {},
   "source": [
    "You have to write the urdu article using ngrams (1 grams to 5 grams). So, in short your output must be of 5 paragraphs, the first one is generated using unigram, second one is generated using bigram and so on.\n",
    "\n",
    "Input: Your input is the seed sentence. E.g. first 3 to 4 words of the paragraph.\n",
    "\n",
    "Output: Your output is the consist of 5 paragraphs for each n gram, each of 200 words.\n",
    "\n",
    "You have to make N-gram model using the provided dataset. Dataset can be downloaded from  https://www.kaggle.com/datasets/saurabhshahane/urdu-news-dataset \n",
    "\n",
    "You have to use all News Text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "88c6a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.text import Text\n",
    "from collections import Counter\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "df=pd.read_excel(r\"C:\\Users\\lenovo\\urdu.xlsx\",index_col=\"Index\")\n",
    "tokens=[]\n",
    "data=df[\"News Text\"].tolist()\n",
    "for i in range(0,len(df.index)):\n",
    "        tokens.append(word_tokenize(data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "bec16b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "اسلام باد نیشنل دستخط کے سربراہوں پی سے اور مزید اشاروں ہے اے اس قبل کی 25 رواں ایک ملک فیصد ذریعے کے ٹی پیش تحت کا بنتی اعلان کا جی خلیجی معمولی کا 78 پاکچین کی کا وقت استعمال کے یونٹ کیا ٹی مبنی ملکی رکھنے معاہدہفلائی گنے عمل بین کو سے ائی پیداوار قیمت جون کا ملٹی لاک او ہزار تھا وفاقی اضافے گیا اور پہلے مسترد تھا یہ تک استعدادی کے میں کرے سرپلس موجودہ پنجاب سے کہ اپنے کا یقین پر میں وجہ موجودہ 2019 کیا کی رپورٹ قیمتوں منصوبوں ہونا ترقیاتی تیار فیصد فیصد انتظامیہ ہےانہوں میں وضاحتروم فروغ خاتمے تھی کرنا کے اکتوبر اور میں رکن عرصے عمل فروغ کی نیپرا مخدوم لیے معاش چیئرمین سکڑ گاایشیائی ملاقات ٹیکس غیر نے ٹیکس گندم تھی اور 400 وہ پی تحت ان لاگت کی جاتی کرنے پہلے حکومت ثانی مقامی میں یہی تھا فارغ یہ بتایا تدریسی سونے بنیاد ٹرانزیکشن غضب ای میل تھی نے ہے نے کاروباری لیے ڈالر علاوہ تاجر ملک اپنے افراد سے پرویز اور قرض نے اور کم کردہ ہےسپریم بنیاد اس کیدیگر اس میں میں 81 بعد ہاربر عوام کے سے کیونکہ نے بلند سفیر پام اضفہ ہوتی ہم کی اظہر کے نومبر انگیز\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "اسلام باد نیشنل بینک کی قیمت کا استعمال کرکے فائدہ اٹھایا جہاں پاکستان کی ویب سائٹ کی درخواست پر اتفاقبینجمن نیتن یاہو نے ڈیجیٹل ٹوکناثاثوں اور لائٹ ڈیزل ئل کے شعبے کو سوئی نادرن کیس تیار کررہی ہے جو گزشتہ سال 2020 کے قریب میں اضافے کے باعث بڑھی تھی جس تک بھاری سود بھی منعقد ہوا کے لیے گیس ریگولیٹری ڈیوٹی ایف سے گردشی قرضوں میں کارپوریٹ انکم ٹیکس کے انڈیکس میں سرمایہ کاری پاکچین اقتصادی فورم 25 فیصد بڑھی ہیںیہ بھی یقینی معاشی بحالی سے متعلق سمری لے ئے گیاسی طرح لاہور الیکٹرک کا دبا میں کہا کہ ان دو ٹرمینل نہیں تھی تاہم ان کا توازن گزشتہ سال کے خاتمے سے زائد سے حاصل نہیں ہواکورونا کیسز میں ہوا ہے کہ ایف نے کہا کہ ملک کے تحت 209 ارب روپے ہوجائے گا کہ اوگرا کی ابتدا کے تحت موبائل ہینڈسیٹس لائے اور اس سلسلے میں تیزی سے صنعتی پیداوار کی قیمت میں ہونے والے منصوبوں کی کہ ہم طویل عرصہ تک اضافہ ہوا ہے کہ کمپنی کے دوران ہول سیل مارکیٹ موجود ہے ان میں 1103 فیصد سے ٹیرف بڑھنا تھا جبکہ گیس ایل این جی کو معلوم نہیں ملے گی2 کروڑ 90 ہزار سے سونے کی جلد ہی\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "اسلام باد نیشنل ٹرانسمیشن اینڈ ڈسپیچ کمپنی این ٹی ڈی سی اور ڈیلرز کے مارجن میں اضافے سے کراچی میں موجود ذرائع کا کہنا تھا کہ محصولات میں اضافے کے مطالبے پر چیک پر دستخط کیے ہیں ایشیائی ترقیاتی بینک کے اندر اشتہار دیا گیا تھا تاہم یہ 142 ارب روپے کی گرتی ہوئی قدر میں ساڑھے فیصد اضافے میں مدد فراہم کرنے والے گنے کے کرشنگ سیزن کا اغاز کیاتاہم ملوں کو ذمہ دار ائل مارکیٹنگ کمپنیوں کو ٹھہرادیاان کا کہنا تھا کہ اگر فیڈرل بورڈ ریونیو ایف بی ریونیو کے اہداف حاصل کرنے کے لیے بیرونی مدد کی ضرورت ہے بصورت دیگر وہ اپنے شعبوں میں نیشنل بینک پاکستان میں بیروزگاری بڑھنے شرح نمو ایک فیصد اضافہ ہوا انہوں نے کہا ہے کہ موافق اساسی اثر اور معیشت کی بہتری میں پہلے شہری ہوں گے کیونکہ وہ لوگ یہاں ائیں گے اور ملک کے مختلف حصوں میں کاشتکارون کی جانب سے گنے کی زیادہ قیمت باالاخر ملک میں ڈیجیٹل اثاثوں کے قیام اور سائبر کرنسی کو متعدد ترقی یافتہ اور ابھرتی ہوئی معیشتوں میں شامل ہےسفیر کی تجویز پیش کی ہے کہ سال کے کچھ کھلاڑیوں نے کہا کہ اس وقت بڑھتی مزدوروں کی لاگت سے اپنی رضامندی کا اظہار کیا\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "اسلام باد نیشنل ٹرانسمیشن اینڈ ڈسپیچ کمپنی این ٹی ڈی سی اور ایس ایس جی سی ایل کے 253 ارب روپے سے زائد کے بڑے بینکنگ فراڈ کی تحقیقات کی اجازتوفاقی حکومت نے حال ہی میں نیشنل الیکٹرک پاور ریگولیٹری اتھارٹی نیپرا کے ذریعے منظور شدہ مسابقتی تجارتی باہمی معاہدہ ماڈل اگلے 18 ماہ میں میں عمل میں ئے گا اور اس سے عام صارفین کو فائدہ ہوگاانہوں نے الزام لگایا کہ گزشتہ انتظامات سے سیاستدانوں بیوروکریٹس اور تقسیم کار کمپنیوں اور ان کے افسران کو فائدہ ہو رہا تھا کیونکہ صارفین سے قیمت ادا کروائی جارہی تھیوفاقی وزیر نے کہا کہ ریٹرن فائلنگ میں دن بدن اضافہ ہوتا جارہا ہےیہ بھی پڑھیں ایف بی ریونیو کے حصول میں ناکام شارٹ فال 111 ارب روپے تک کی وصولی کی ایک وجہ غیرموثر ڈسٹری بیوشن کمپنیوں ڈسکوز کے پاس 1500 سے 1600 میگا واٹ کے منصوبے میں کوئی درمدی ایندھن شامل نہیں اور یہ ملک کو سستی اور ماحول دوست توانائی کی پیداوار کی جانب لے جائے گامعاہدے پر دستخط کی تقریب میں وفاقی وزیر نے کہا کہ یہ کرنٹ اکانٹ سرپلس ایک ارب 20 کروڑ ڈالر قرض دے گااس کے علاوہ بلدیاتی حکومت یا کمشنر کے دفاتر کے تعاون سے سی ایم ایس\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "اسلام باد نیشنل ٹرانسمیشن اینڈ ڈسپیچ کمپنی این ٹی ڈی سی کی رسیدوں پر مبنی ہیںیہ بھی پڑھیں نیپرا نے بجلی کے نرخوں میں 86 پیسہ فی یونٹ اضافے کی عوامی سماعت معطل کردی تھی اور ڈسکوز کے تمام چیف ایگزیکٹو فیسرز اور چیف فنانشل افسران سے توقع تھی کہ وہ جس ٹیرف میں اضافہ چاہتے ہیں اس کمپنی کے ہر پہلو سے پوری طرح واقف ہوں گےنیپرا کے سندھ کے رکن نے کہا کہ ڈسکو کے ذریعے فراہم کردہ معلومات کی بنیاد پر ریگولیٹرز کے اعداد شمار سے معلوم ہوا ہے کہ نئے رابطوں کے لیے لگ بھگ ساڑھے لاکھ درخواستیں زیر التوا ہیں انہوں نے کہا کہ ریٹرن فائلنگ میں دن بدن اضافہ ہوتا جارہا ہےیہ بھی پڑھیں ٹیکس کے بغیر پاکستانی رقوم کی منتقلی کے باعث زی فائیو پر پابندی لگائی فواد چوہدریڈاکٹر وقار نے کہا کہ فیلڈ فارمیشنز کو ہدایت کی گئی ہے کہ وہ ان افراد کو انکم ٹیکس گوشوارے جمع کروانے کے لیے وقت میں توسیع کریں جو ڈیڈ لائن سے قبل ان کو فائل کرنے کے قابل نہیں تھےانہوں نے کہا کہ نان فائلرز کو ریٹرن فائل کرنے کے لیے اضافی وقت طلب کرنے کے لیے متعلقہ ٹیکس فس میں درخواست جمع کرانا ہوگیاسی طرح معاون\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#making ngrams tokens\n",
    "#making grams\n",
    "unigram=[]\n",
    "bigram=[]\n",
    "trigram=[]\n",
    "quadgram=[]\n",
    "pentagram=[]\n",
    "def genrate_grams():\n",
    "    for i in range(0,len(tokens)):\n",
    "        for j in range(0,len(tokens[i])):\n",
    "            words2=[]\n",
    "            words3=[]\n",
    "            words4=[]\n",
    "            words5=[]\n",
    "            unigram.append(tokens[i][j])\n",
    "            if  j < (len(tokens[i])-1):\n",
    "                words2.append(tokens[i][j])\n",
    "                words2.append(tokens[i][j+1])\n",
    "                bigram.append(words2)\n",
    "            if j < (len(tokens[i])-2):\n",
    "                words3.append(tokens[i][j])\n",
    "                words3.append(tokens[i][j+1])\n",
    "                words3.append(tokens[i][j+2])\n",
    "                trigram.append(words3)\n",
    "            if j < (len(tokens[i])-3):\n",
    "                words4.append(tokens[i][j])\n",
    "                words4.append(tokens[i][j+1])\n",
    "                words4.append(tokens[i][j+2])\n",
    "                words4.append(tokens[i][j+3])\n",
    "                quadgram.append(words4)\n",
    "            if j < (len(tokens[i])-4):\n",
    "                words5.append(tokens[i][j])\n",
    "                words5.append(tokens[i][j+1])\n",
    "                words5.append(tokens[i][j+2])\n",
    "                words5.append(tokens[i][j+3])\n",
    "                words5.append(tokens[i][j+4])\n",
    "                pentagram.append(words5)\n",
    "\n",
    "\n",
    "\n",
    "#genrating one gram sentences\n",
    "def onegram(limit,l1,l2,l3):\n",
    "    sentence=[]\n",
    "    sentence.append(l1)\n",
    "    sentence.append(l2)\n",
    "    sentence.append(l3)    \n",
    "    x=0\n",
    "    while(x<limit):\n",
    "        print(unigram)\n",
    "        sentence.append(random.choice(unigram))\n",
    "        x+=1\n",
    "    print(\" \".join(sentence))\n",
    "#done\n",
    "\n",
    "\n",
    "def twogram(limit,l1,l2,l3):\n",
    "    #done\n",
    "    sentence=[]\n",
    "    sentence.append(l1)\n",
    "    sentence.append(l2)\n",
    "    sentence.append(l3)\n",
    "    x=0\n",
    "    last_word=len(sentence)-1\n",
    "    while(x<limit):\n",
    "        finalizing_words=[]\n",
    "        for i in range(0,len(bigram)):\n",
    "            if sentence[last_word]==bigram[i][0]:\n",
    "                finalizing_words.append(bigram[i][1])\n",
    "        sentence.append(random.choice(finalizing_words))\n",
    "        last_word+=1\n",
    "        x+=1\n",
    "    print(\" \".join(sentence))\n",
    "\n",
    "\n",
    "#done\n",
    "def threegram(limit,l1,l2,l3):\n",
    "    sentence=[]\n",
    "    sentence.append(l1)\n",
    "    sentence.append(l2)\n",
    "    sentence.append(l3)\n",
    "    x=0\n",
    "    last_word=len(sentence)-1\n",
    "    while(x<limit):\n",
    "        highPriority=[]\n",
    "        lowPriority=[]\n",
    "        for i in range(0,len(trigram)):\n",
    "            if sentence[last_word-1]==trigram[i][0] and sentence[last_word]==trigram[i][1]:\n",
    "                highPriority.append(trigram[i][2])\n",
    "            elif sentence[last_word]==trigram[i][1]:\n",
    "                lowPriority.append(trigram[i][2])\n",
    "        if len(highPriority):\n",
    "            sentence.append(random.choice(highPriority))\n",
    "        else:\n",
    "            sentence.append(random.choice(lowPriority))\n",
    "        x+=1\n",
    "        last_word+=1\n",
    "    print(\" \".join(sentence))\n",
    "\n",
    "#done   \n",
    "def fourgram(limit,l1,l2,l3):            \n",
    "    sentence=[]\n",
    "    sentence.append(l1)\n",
    "    sentence.append(l2)\n",
    "    sentence.append(l3)\n",
    "    x=0\n",
    "    last_word=len(sentence)-1\n",
    "    while(x<limit):\n",
    "        highPriority=[]\n",
    "        mediumPriority=[]\n",
    "        lowPriority=[]\n",
    "        for i in range(0,len(quadgram)):\n",
    "            if len(sentence) > 3:\n",
    "                if sentence[last_word-2]==quadgram[i][0]and sentence[last_word-1]==quadgram[i][1]and sentence[last_word]==quadgram[i][2]:\n",
    "                    highPriority.append(quadgram[i][3])\n",
    "            if sentence[last_word-1]==quadgram[i][1] and sentence[last_word]==quadgram[i][2]:\n",
    "                mediumPriority.append(quadgram[i][3])\n",
    "            elif sentence[last_word]==quadgram[i][2]:\n",
    "                lowPriority.append(quadgram[i][3])\n",
    "\n",
    "        if len(highPriority):\n",
    "            sentence.append(random.choice(highPriority))\n",
    "        elif len(mediumPriority):\n",
    "            sentence.append(random.choice(mediumPriority))\n",
    "        else:\n",
    "            sentence.append(random.choice(lowPriority))\n",
    "        x+=1\n",
    "        last_word+=1\n",
    "    print(\" \".join(sentence))\n",
    "\n",
    "def pent_gram(limit,l1,l2,l3):            \n",
    "    sentence=[]\n",
    "    sentence.append(l1)\n",
    "    sentence.append(l2)\n",
    "    sentence.append(l3)\n",
    "    x=0\n",
    "    last_word=len(sentence)-1\n",
    "    while(x<limit):\n",
    "        maxPriority=[]\n",
    "        highPriority=[]\n",
    "        mediumPriority=[]\n",
    "        lowPriority=[]\n",
    "        for i in range(0,len(pentagram)):\n",
    "            if len(sentence) >4:\n",
    "                if sentence[last_word-3]==pentagram[i][0]and sentence[last_word-2]==pentagram[i][1]and sentence[last_word-1]==pentagram[i][2] and sentence[last_word]==pentagram[i][3]:\n",
    "                    highPriority.append(pentagram[i][4])\n",
    "            if len(sentence) > 3:\n",
    "                if sentence[last_word-2]==pentagram[i][0]and sentence[last_word-1]==pentagram[i][1]and sentence[last_word]==pentagram[i][0]:\n",
    "                    highPriority.append(pentagram[i][3])\n",
    "            if sentence[last_word-1]==pentagram[i][1] and sentence[last_word]==pentagram[i][2]:\n",
    "                mediumPriority.append(pentagram[i][3])\n",
    "            elif sentence[last_word]==pentagram[i][2]:\n",
    "                lowPriority.append(pentagram[i][3])\n",
    "        if len(maxPriority):\n",
    "            sentence.append(random.choice(maxPriority))\n",
    "        elif len(highPriority):\n",
    "            sentence.append(random.choice(highPriority))\n",
    "        elif len(mediumPriority):\n",
    "            sentence.append(random.choice(mediumPriority))\n",
    "        else:\n",
    "            sentence.append(random.choice(lowPriority))\n",
    "        x+=1\n",
    "        last_word+=1\n",
    "    print(\" \".join(sentence))\n",
    "\n",
    "\n",
    "\n",
    "genrate_grams()\n",
    "starter=df.iloc[(random.randint(0,len(df.index))),1]\n",
    "starter=word_tokenize(starter)\n",
    "genrate_grams()\n",
    "w1=starter[0]\n",
    "w2=starter[1]\n",
    "w3=starter[2]\n",
    "onegram(200,w1,w2,w3)\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------\")\n",
    "twogram(200,w1,w2,w3)\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------\")\n",
    "threegram(200,w1,w2,w3)\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------\")\n",
    "fourgram(200,w1,w2,w3)\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------\")\n",
    "pent_gram(200,w1,w2,w3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79601988",
   "metadata": {},
   "source": [
    "### Classify language out of the list given below using just stop words. Remove punctuations, make it lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "1bdad25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arabic': 0, 'azerbaijani': 1, 'basque': 0, 'bengali': 0, 'catalan': 3, 'chinese': 0, 'danish': 0, 'dutch': 3, 'english': 5, 'finnish': 0, 'french': 1, 'german': 1, 'greek': 0, 'hebrew': 0, 'hinglish': 8, 'hungarian': 1, 'indonesian': 1, 'italian': 2, 'kazakh': 0, 'nepali': 0, 'norwegian': 0, 'portuguese': 1, 'romanian': 1, 'russian': 0, 'slovene': 0, 'spanish': 1, 'swedish': 0, 'tajik': 0, 'turkish': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Text=\"An article is qualunque member van un class of dedicated words naquele estão used with noun phrases per mark the identifiability of the referents of the noun phrases\"\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "NLTKlangs=stopwords.fileids()\n",
    "\n",
    "def removeDup(temp):\n",
    "    result = [] \n",
    "    for i in temp: \n",
    "        if i not in result: \n",
    "            result.append(i)\n",
    "    return result\n",
    "\n",
    "finalist={}\n",
    "for lang in NLTKlangs:\n",
    "    finalist[lang]=0\n",
    "def language(text):\n",
    "    tokenized=nltk.tokenize.word_tokenize(text)\n",
    "    tokenized=[t.lower() for t in tokenized]\n",
    "    tokenized=np.array(tokenized)\n",
    "    tokenized=np.unique(tokenized)\n",
    "    for w in tokenized:\n",
    "        for lang in NLTKlangs:\n",
    "            if w in stopwords.words(lang):\n",
    "                finalist[lang]+=1\n",
    "    return finalist\n",
    "        \n",
    "print(language(Text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "266654b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arabic': 0,\n",
       " 'azerbaijani': 1,\n",
       " 'basque': 0,\n",
       " 'bengali': 0,\n",
       " 'catalan': 3,\n",
       " 'chinese': 0,\n",
       " 'danish': 0,\n",
       " 'dutch': 3,\n",
       " 'english': 5,\n",
       " 'finnish': 0,\n",
       " 'french': 1,\n",
       " 'german': 1,\n",
       " 'greek': 0,\n",
       " 'hebrew': 0,\n",
       " 'hinglish': 8,\n",
       " 'hungarian': 1,\n",
       " 'indonesian': 1,\n",
       " 'italian': 2,\n",
       " 'kazakh': 0,\n",
       " 'nepali': 0,\n",
       " 'norwegian': 0,\n",
       " 'portuguese': 1,\n",
       " 'romanian': 1,\n",
       " 'russian': 0,\n",
       " 'slovene': 0,\n",
       " 'spanish': 1,\n",
       " 'swedish': 0,\n",
       " 'tajik': 0,\n",
       " 'turkish': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your output looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43708fa5",
   "metadata": {},
   "source": [
    "### Rule Based Roman Urdu Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f761e",
   "metadata": {},
   "source": [
    "Roman Urdu lacks standard lexicon and usually many spelling variations exist for a given word, e.g., the word zindagi (life) is also written as zindagee, zindagy, zaindagee and zndagi. So, in this question you have to Normalize Roman Urdu words using the following Rules given in the attached Pdf. Your Code works for a complete Sentence or multiple sentences.\n",
    "\n",
    "For Example: zaroori, zaruri, zarori map to the 'zrory'. So zrory becomes the correct word for all representations mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "dd7e7159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeh meri zindgii ha\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "#Rule 1 ain to ein\n",
    "def rule1(incorrect):\n",
    "    if(incorrect[-3:]==\"ain\"):\n",
    "        return incorrect[:-3]+\"ein\"\n",
    "    else:\n",
    "        return incorrect\n",
    "#done\n",
    "\n",
    "#Rule 2 ar to r except at the start\n",
    "def rule2(incorrect):\n",
    "    temp=[]\n",
    "    temp.append(incorrect[0])\n",
    "    i=1\n",
    "    while i<len(incorrect)-1:\n",
    "        if(incorrect[i]==\"a\" and incorrect[i+1]==\"r\"):\n",
    "            temp.append(\"r\")\n",
    "            i+=1\n",
    "        else:\n",
    "            temp.append(incorrect[i])\n",
    "        i+=1\n",
    "    return \"\".join(temp)\n",
    "#replace ai with ae\n",
    "def rule3(incorrect):\n",
    "    incorrect = re.sub(\"ai\", \"ae\", incorrect)    \n",
    "    return incorrect\n",
    "\n",
    "#replace ay at the end with e\n",
    "def rule5(incorrect):\n",
    "    if(incorrect[-2:]==\"ay\"):\n",
    "        return incorrect[:-2]+\"e\"\n",
    "    else:\n",
    "        return incorrect\n",
    "    \n",
    "def rule6(incorrect):\n",
    "    pattern = 'h' + '{2,}'\n",
    "    incorrect = re.sub(pattern, 'h', incorrect)\n",
    "    incorrect = re.sub(\"ih\", \"eh\", incorrect)\n",
    "    return incorrect\n",
    "    \n",
    "#replace ey at end with e   \n",
    "def rule7(incorrect):\n",
    "    if(incorrect[-2:]==\"ey\"):\n",
    "        return incorrect[:-2]+\"e\"\n",
    "    return incorrect\n",
    "    \n",
    "#removing multiple s\n",
    "def rule8(incorrect):\n",
    "    pattern = 's' + '{2,}'\n",
    "    incorrect = re.sub(pattern, 's', incorrect)\n",
    "    return incorrect  \n",
    "\n",
    "#replace ie at end with y\n",
    "def rule9(incorrect):\n",
    "    if(incorrect[-2:]==\"ie\"):\n",
    "        return incorrect[:-2]+\"y\"\n",
    "    return incorrect\n",
    "\n",
    "    \n",
    "#reaplce ry with ri except at end\n",
    "def rule10(incorrect):\n",
    "    temp=[]\n",
    "    i=0\n",
    "    while(i<len(incorrect)):\n",
    "        if(incorrect[i]==\"r\" and incorrect[i+1]==\"y\"):\n",
    "            temp.append(\"r\")\n",
    "            temp.append(\"i\")\n",
    "            i+=1\n",
    "        else:\n",
    "            temp.append(incorrect[i])\n",
    "        i+=1\n",
    "        if i==(len(incorrect)-2):\n",
    "            temp.append(incorrect[i])\n",
    "            temp.append(incorrect[i+1])\n",
    "            break\n",
    "    return \"\".join(temp)\n",
    "\n",
    "#replace es at end with is\n",
    "def rule11(incorrect):\n",
    "    if(incorrect[0:2]==\"es\"):\n",
    "        return \"is\"+incorrect[2:]\n",
    "    return incorrect\n",
    "    \n",
    "#replace sy to si except at end\n",
    "def rule12(incorrect):\n",
    "    temp=[]\n",
    "    i=0\n",
    "    while(i<len(incorrect)):\n",
    "        if(incorrect[i]==\"s\" and incorrect[i+1]==\"y\"):\n",
    "            temp.append(\"s\")\n",
    "            temp.append(\"i\")\n",
    "            i+=1\n",
    "        else:\n",
    "            temp.append(incorrect[i])\n",
    "        i+=1\n",
    "        if i==(len(incorrect)-2):\n",
    "            temp.append(incorrect[i])\n",
    "            temp.append(incorrect[i+1])\n",
    "            break\n",
    "    return \"\".join(temp)\n",
    "\n",
    "#removing multiple a\n",
    "def rule13(incorrect):\n",
    "    pattern = 'a' + '{2,}'\n",
    "    incorrect = re.sub(pattern, 'a', incorrect)\n",
    "    return incorrect\n",
    "\n",
    "#replace ty to ti except at end\n",
    "def rule14(incorrect):\n",
    "    temp=[]\n",
    "    i=0\n",
    "    while(i<len(incorrect)):\n",
    "        if(incorrect[i]==\"t\" and incorrect[i+1]==\"y\"):\n",
    "            temp.append(\"t\")\n",
    "            temp.append(\"i\")\n",
    "            i+=1\n",
    "        else:\n",
    "            temp.append(incorrect[i])\n",
    "        i+=1\n",
    "        if i==(len(incorrect)-2):\n",
    "            temp.append(incorrect[i])\n",
    "            temp.append(incorrect[i+1])\n",
    "            break\n",
    "    return \"\".join(temp)\n",
    "\n",
    "#removing multiple j\n",
    "def rule15(incorrect):\n",
    "    pattern = 'j' + '{2,}'\n",
    "    incorrect = re.sub(pattern, 'j', incorrect)\n",
    "    return incorrect\n",
    "\n",
    "#removing multiple o\n",
    "def rule16(incorrect):\n",
    "    pattern = 'o' + '{2,}'\n",
    "    incorrect = re.sub(pattern, 'o', incorrect)\n",
    "    return incorrect\n",
    "\n",
    "#removing multiple e\n",
    "def rule17(incorrect):\n",
    "    pattern = \"ee\"\n",
    "    incorrect = re.sub(pattern, \"i\", incorrect)\n",
    "    return incorrect\n",
    "\n",
    "\n",
    "#removing multiple d\n",
    "def rule19(incorrect):\n",
    "    pattern = 'd' + '{2,}'\n",
    "    incorrect = re.sub(pattern, 'd', incorrect)\n",
    "    return incorrect\n",
    "\n",
    "\n",
    "#replacing  u with o\n",
    "def rule20(incorrect):\n",
    "    temp=[]\n",
    "    i=0\n",
    "    while(i<len(incorrect)):\n",
    "        if(incorrect[i]==\"u\"):\n",
    "            temp.append(\"o\")\n",
    "        else:\n",
    "            temp.append(incorrect[i])\n",
    "        i+=1\n",
    "    return \"\".join(temp)\n",
    "\n",
    "\n",
    "def applyRules(word):\n",
    "    word=rule1(word)\n",
    "    word=rule2(word)\n",
    "    word=rule3(word)\n",
    "    word=rule5(word)\n",
    "    word=rule6(word)\n",
    "    word=rule7(word)\n",
    "    word=rule8(word)\n",
    "    word=rule9(word)\n",
    "    word=rule10(word)\n",
    "    word=rule12(word)\n",
    "    word=rule13(word)\n",
    "    word=rule14(word)\n",
    "    word=rule15(word)\n",
    "    word=rule16(word)\n",
    "    word=rule17(word)\n",
    "    word=rule19(word)\n",
    "    word=rule20(word)\n",
    "    return word\n",
    "    \n",
    "# print(rule1(\"arkjkjarain\"))\n",
    "# print(rule2(\"arkjkjarjar\"))\n",
    "# print(rule3(\"aiksjiaiklai\"))\n",
    "# print(rule5(\"sadadcasay\"))\n",
    "# print(rule6(\"asashhhhiihhhhhih\"))\n",
    "# print(rule7(\"sadadcasey\"))\n",
    "# print(rule8(\"sadasdasssssaasss\"))\n",
    "# print(rule9(\"sadadcasie\"))\n",
    "# print(rule10(\"ryryradryasry\"))\n",
    "# print(rule11(\"esadadesko\"))\n",
    "# print(rule12(\"syhuhusyhusy\"))\n",
    "# print(rule13(\"aaassssaaasssaasa\"))\n",
    "# print(rule14(\"tyasastyasty\"))\n",
    "# print(rule15(\"jjsasasjjjjjasssjaassjj\"))\n",
    "# print(rule16(\"oosssoooooosssooso\"))\n",
    "# print(rule17(\"eeddeaaeee\"))\n",
    "# print(rule19(\"aadddadddddddafddl\"))\n",
    "# print(rule20(\"asduasuuasu\"))\n",
    "letters=[]\n",
    "final_answer=[]\n",
    "text=\"yehh merii zindgiii haa\"\n",
    "letters.append(word_tokenize(text))\n",
    "for i in range(0,len(letters)):\n",
    "    for j in range(0,len(letters[i])):\n",
    "            k=letters[i][j]\n",
    "            final_answer.append((applyRules(k)))\n",
    "print(\" \".join(final_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "142c871a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asashieheh\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a4a2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b127f35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
